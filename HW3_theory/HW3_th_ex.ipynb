{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Unsupervised Learning\n",
    "\n",
    "# Part 1: Classification with K-means algorithm\n",
    "\n",
    "The K-means algorithm is a fundamental tool among the unsupervised learning model. Consider a problem with a dataset $\\mathcal{D} = \\left\\{x_i\\right\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ with no labels, we are aiming at finding some hidden structure within the data, namely, we would like to find clusters in the dataset. Classifiers have been studied in TP but mainly for supervised learning, here the data are not labeled. \n",
    "\n",
    "The K-means algorithm tries to classify the dataset in $K$ clusters. Each cluster is represented by a centroid, meaning the average of the points within the cluster. We note $C_k$ the set of points of a cluster $k$ and $\\mu_k$ its centroid. Then, the algorithm minimizes the intra-cluster variance, in other words, it tries to reduce the distance between the points of the cluster and the centroid. \n",
    "\n",
    "More technically, the algorithm works iteratively in two main steps: \n",
    " - Points are assigned to clusters based on their proximity to existing centroid\n",
    " - Centroids are updated by taking the average of the points assigned to each cluster.\n",
    "\n",
    "\n",
    "\n",
    "The Loss function of the problem can be written as:\n",
    "$$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 \\; ,$$\n",
    "where $\\mu(i)$ is the centroid of the cluster assignated to $x_i$. \n",
    "\n",
    "We want to check the understanding of the choice of this loss function. Does it match the aforementioned rules? Then, could it help to understand if the algorithm converges?\n",
    "\n",
    "Additionnally, until Question 8, we assume that the clusters $C_k$ are disjoint, especially at initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "First, let us familiarize ourselves with the loss function:\n",
    "\n",
    " - Prove the second equality:\n",
    "\n",
    "   $$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$$\n",
    "\n",
    " - What does the term $\\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$ represent?\n",
    " \n",
    " - Explain why this form of the loss function is more convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that \\mu_{C(i)} is the centroid of the cluster C_k containing x_i, which means that when summing over n, it is equivalent to summing over the K clusters once each x_i is assigned to those clusters.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\|x_i - \\mu_{C(i)}\\|^2\n",
    "\\;=\\;\n",
    "\\sum_{k=1}^K \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2\n",
    "$$\n",
    "\n",
    "Since $ x_i \\in C_k $ implies $ \\mu_{C(i)} = \\mu_k $. So when we sum over the clusters, this holds for all $x_i \\in C_k, k = 1, \\dots, K.$\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\n",
    "\\sum_{i=1}^n \\|x_i - \\mu_{C(i)}\\|^2\n",
    "=\n",
    "\\sum_{k=1}^K\n",
    "\\sum_{x_i \\in C_k}\n",
    "\\|x_i - \\mu_k\\|^2.\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The term\n",
    "$\\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2$\n",
    "\n",
    "represents the distance between a point $x_i$ and its centroid within a cluster, of course, and the sum over all $x_i$ within the cluster represents the loss function for this cluster.\n",
    "\n",
    "This form is more convenient , let‚Äôs say more explicit, than the other because it enables us to see more clearly what is going on.\n",
    "\n",
    "In the first place we compare the loss within a cluster k, and then we sum all losses of all clusters, which is more convenient to deal with in terms of optimization.\n",
    "\n",
    "You can clearly observe what each cluster is making us lose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Let us focus on the first point of the algorihtm, let us consider a single point $x_i$ and add the time dependency. Also, we denote by a $'$ the variables after the new assignement of the data points. \n",
    "\n",
    "So that, the variables are denoted by: $(\\cdot)^t \\to (\\cdot)^t\\,{}' \\to (\\cdot)^{t+1} \\to (\\cdot)^{t+1}\\,{}' \\to (\\cdot)^{t+2}$\n",
    "\n",
    "\n",
    "Thus, at each step time $t$, the new assignements of the variables leads to: (no proof required)\n",
    "\n",
    "$$ \\mu^t(i)' = {\\rm argmin}_{\\mu \\in \\left\\{\\mu^t_k\\right\\}_k} \\lVert x_i - \\mu \\rVert^2$$\n",
    "\n",
    "For instance, at time $t$ before new assignements, the vector $x_i$ belongs to a cluster $k$, while after the next assignement, it now belongs to the cluster $k'$ (it can be the same or different from the cluster $k$). \n",
    "\n",
    "\n",
    "Thus, for the whole dataset, the algorithm updates the assignement as: $\\left\\{\\mu^t(i) \\right\\} \\to \\left\\{\\mu^t(i)'\\right\\}$.\n",
    "\n",
    "\n",
    "Compare $\\lVert x_i - \\mu^t(i) \\rVert^2$ and $\\lVert x_i - \\mu^t(i)' \\rVert^2$ for a given point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained, we have different cases.\n",
    "\n",
    "Case 1 ‚Äî The centroid does not change\n",
    "\n",
    "If after the update, the centroid \\mu_{C(i)}^t is still the same:\n",
    "\n",
    "$\\|x_i - \\mu_{C(i)}^{t'}\\|^2 = \\|x_i - \\mu_{C(i)}^t\\|^2$.\n",
    "\n",
    "Case 2 ‚Äî The centroid changes\n",
    "\n",
    "The cluster is updated, and in this case \\mu_{C(i)}^{t+1} is closer to x_i than \\mu_{C(i)}^t.\n",
    "So we have:\n",
    "\n",
    "$\\|x_i - \\mu_{C(i)}^{t'}\\|^2 \\le \\|x_i - \\mu_{C(i)}^t\\|^2$.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "In general:\n",
    "\n",
    "$\\|x_i - \\mu_{C(i)}^{t'}\\|^2 \\le \\|x_i - \\mu_{C(i)}^{t}\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "We recall that we denote by a $'$ the variables after the new assignement of the data points, so that: $\\mu^t(i) \\to \\mu^t(i)'$ and $J_t \\to J_t'$\n",
    "\n",
    "Thanks to the previous question, compare $J_t$ and ${J_t}'$. \n",
    "\n",
    "Hint: Pick the right formula between the two given for $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous question we said:\n",
    "\n",
    "$$\n",
    "\\|x_i - \\mu_{C(i)}^{t'}\\|^2\n",
    "\\le\n",
    "\\|x_i - \\mu_{C(i)}^t\\|^2.\n",
    "$$\n",
    "\n",
    "Elevating to the sum within a cluster:\n",
    "\n",
    "$$\n",
    "\\sum_{x_i \\in C_k^{t'}} (x_i - \\mu_k^{t'})^2\n",
    "\\;\\;\\le\\;\\;\n",
    "\\sum_{x_i \\in C_k^t} (x_i - \\mu_k^{t})^2.\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "${J_t}' \\le J_t$.\n",
    "\n",
    "Which proves monotonic decrease of the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead by studying the second point of the algorithm. It wants to minimize the intra-cluster variance:\n",
    "\n",
    "$$ \\left\\{\\mu^{t+1}_k\\right\\}_k = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} J'_t\\left( \\left\\{\\mu_k\\right\\}_k \\right) = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$\n",
    "\n",
    "\n",
    "Show that the optimization can be done cluster-wise:\n",
    "\n",
    "$$\\mu^{t+1}_k = {\\rm argmin}_{\\mu_k \\in \\mathbb{R}^d}  \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Question 4**\n",
    "\n",
    "Here we study the 2nd step of the algorithm, which is updating the centroid within a cluster.\n",
    "\n",
    "Our goal with this step is to minimize the intra-cluster variance, so it is clear that the operation we perform is clusterwise.\n",
    "\n",
    "Let‚Äôs rewrite the function:\n",
    "\n",
    "$$\n",
    "\n",
    "\\{\\mu_k^{t+1}\\}_{k=1}^K\n",
    "=\n",
    "\\arg\\min_{\\{\\mu_k\\} \\in \\mathbb{R}^d}\n",
    "\\sum_{k=1}^K\n",
    "\\sum_{x_i \\in C_k^t}\n",
    "\\|x_i - \\mu_k\\|^2.\n",
    "\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\n",
    "\\{\\mu_k^{t+1}\\}_k\n",
    "=\n",
    "\\arg\\min_{(\\mu_k)_k \\in \\mathbb{R}^d}\n",
    "\\sum_{k=1}^K\n",
    "J_k(\\mu_k)\n",
    "\n",
    "$$\n",
    "\n",
    "Let‚Äôs consider one cluster:\n",
    "\n",
    "$$\n",
    "\n",
    "J_k(\\mu_k)\n",
    "=\n",
    "\\sum_{x_i \\in C_k^t}\n",
    "\\|x_i - \\mu_k\\|^2.\n",
    "\n",
    "$$\n",
    "\n",
    "We see that $J_k$ depends only on $\\mu_k$, not on any $\\mu_j$ with $j \\neq k$.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\n",
    "\\mu_k^{t+1}\n",
    "=\n",
    "\\arg\\min_{\\mu_k \\in \\mathbb{R}^d}\n",
    "\\sum_{x_i \\in C_k^t}\n",
    "\\|x_i - \\mu_k\\|^2.\n",
    "\n",
    "$$\n",
    "\n",
    "Which is solved by the sample mean:\n",
    "\n",
    "$\\mu_k^{t+1} = \\frac{1}{|C_k^t|}\\sum_{x_i \\in C_k^t}x_i.$\n",
    "\n",
    "When we minimize over all ${\\mu_k}$ , we're really choosing each $\\mu_k$ in $R^d$  freely.\n",
    "\n",
    "Then we can really do : ${\\mu_k^{t+1}}_k = f_1(\\mu_1) + f_2(\\mu_2) + \\cdots + f_K(\\mu_K)$,\n",
    "\n",
    "\n",
    "So it comes to minimize each function $f_k(\\mu_k)$ independantly so cluster-wise.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 5\n",
    "\n",
    "Show that the new centroids of time $t+1$ are computed according to the following equality:\n",
    "\n",
    "$$ \\mu_k^{t+1} = \\frac{1}{|C^t_k{}'|}\\sum_{x_i \\in C^t_k{}'} x_i $$\n",
    "\n",
    "Does it correspond to what you expected from the algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Question 5**\n",
    "\n",
    "We want to update the centroid \\mu_k by minimizing the intra-cluster variance:\n",
    "\n",
    "$\\mu_k^{t+1} = \\arg\\min_{\\mu_k \\in \\mathbb{R}^d}\\sum_{x_j \\in C_k^{t'}} \\|x_j - \\mu_k\\|^2.$\n",
    "\n",
    "Consider\n",
    "\n",
    "$f_k(\\mu_k) = \\sum_{x_j \\in C_k^{t'}} \\|x_j - \\mu_k\\|^2$.\n",
    "\n",
    "Expand the squared norm:\n",
    "\n",
    "$\\|x_j - \\mu_k\\|^2 = (x_j - \\mu_k)^T (x_j - \\mu_k) = x_j^T x_j - 2 x_j^T \\mu_k + \\mu_k^T \\mu_k$.\n",
    "\n",
    "Thus\n",
    "\n",
    "$f_k(\\mu_k)= \\sum_j x_j^T x_j- 2 \\sum_j x_j^T \\mu_k+ \\sum_j \\mu_k^T \\mu_k$.\n",
    "\n",
    "The first term does not depend on \\mu_k.\n",
    "\n",
    "The last term is\n",
    "\n",
    "$\\sum_j \\mu_k^T \\mu_k= |C_k|\\, \\mu_k^T \\mu_k$.\n",
    "\n",
    "So we rewrite:\n",
    "\n",
    "$f_k(\\mu_k)= \\text{const}- 2 \\left( \\sum_j x_j \\right)^T \\mu_k+ |C_k|\\, \\mu_k^T \\mu_k$.\n",
    "\n",
    "Compute the gradient:\n",
    "\n",
    "$\\nabla_{\\mu_k} f_k(\\mu_k)= -2 \\sum_j x_j + 2 |C_k|\\, \\mu_k$.\n",
    "\n",
    "Set it to zero:\n",
    "\n",
    "$-2 \\sum_j x_j + 2 |C_k|\\, \\mu_k = 0$.\n",
    "\n",
    "Divide by 2:\n",
    "\n",
    "$-|C_k|\\, \\mu_k + \\sum_j x_j = 0$.\n",
    "\n",
    "So\n",
    "\n",
    "$|C_k|\\, \\mu_k = \\sum_{x_j \\in C_k} x_j$.\n",
    "\n",
    "Therefore, the minimizer is:\n",
    "\n",
    "$\\boxed{\\mu_k^{t+1}= \\frac{1}{|C_k^{t'}|} \\sum_{x_j \\in C_k^{t'}} x_j}$\n",
    "\n",
    "which is the mean of all points in cluster C_k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "If we focus on a cluster $k$ at time $t$ after the assignement, noted $C^t_k {}'$, could you compare $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^t \\rVert^2$ and $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^{t+1} \\rVert^2$ ?\n",
    "\n",
    "What can you say about ${J_t}'$ and $J_{t+1}$ ? Hint: Use the right formula between the two given for $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Question 6**\n",
    "\n",
    "Yes, we can  compare the two losses.\n",
    "\n",
    "After the update, $\\mu_k^{t+1}$ is chosen by minimizing distance to all points:\n",
    "\n",
    "$\n",
    "\\sum_{x_i \\in C_k^{t'}} \\| x_i - \\mu_k^{t+1}\\|^2 \\le\\sum_{x_i \\in C_k^{t'}} \\| x_i - \\mu_k^{t}\\|^2.\n",
    "$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$J^{t+1} \\le J^{t'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7:\n",
    "\n",
    "Putting together the Questions 3 and 5, compare $J_t$ and $J_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Question 7** \n",
    "\n",
    "From Question 3, we know:\n",
    "\n",
    "$J_{t'} \\le J_t$.\n",
    "\n",
    "From Question 6, we know:\n",
    "\n",
    "$J_{t+1} \\le J_{t'}$.\n",
    "\n",
    "Combining:\n",
    "\n",
    "$J_{t+1} \\le J_{t'} \\le J^t$.\n",
    "\n",
    "Thus the loss decreases at every iteration.\n",
    "\n",
    "Since:\n",
    "\n",
    "$\\mu_k^{t+1} = \\frac{1}{|C_k^{t'}|} \\sum_{x_i \\in C_k^{t'}} x_i$\n",
    "\n",
    "the new centroid is always closest (on average) to all points in its cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:\n",
    "\n",
    "After recalling a trivial lower bound for the sequence $(J_t)_{t \\geq 0}$, what can you say about the convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8** \n",
    "\n",
    "In the K-means algorithm, we have seen 3 iterations\n",
    "$\\mu^t \\to \\mu^{t'} \\to \\mu^{t+1}$\n",
    "represented by their cost function\n",
    "$J_t \\to J_{t'} \\to J_{t+1}$.\n",
    "\n",
    "At each iteration, the goal is to make the most compact clusters possible.\n",
    "So from what we compare:\n",
    "$J_{t+1} \\le J_{t'} \\le J_t$.\n",
    "Then the lower bound for this algorithm  which is the ‚Äúperfect‚Äù case  is 0, which implies $x_i = \\mu_k$ for every point in its own cluster.\n",
    "\n",
    "In practice, we will never reach this value because among a group (cluster), different points are separated, and cluster centroids are mean points, not exact copies.\n",
    "But as the iterations go on, we will reach a moment when in reality the lower bound is where:\n",
    "$\\mu_k^{t} = \\mu_k^{t+1}$\n",
    "and then we stop.\n",
    "\n",
    "Equivalently, at this moment:\n",
    "$J_t = J_{t'} = J_{t+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 9:\n",
    "\n",
    "We just proved that the algorithm converges, but what about its stability:\n",
    "\n",
    "Let us suppose that the data are sampled from a mixture of $K$ Gaussian, where the choice of $K$ is free for this question. Do you imagine a situation where the algorithm does not classify the data at all? Please design and explain the situation as clearly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of a situation where the Gaussians are well separated.\n",
    "\n",
    "Imagine that there are 3 Gaussians, but we initialize the wrong number of clusters at the wrong places.\n",
    "This could create:\n",
    "\t‚Ä¢\tmore clusters than needed,\n",
    "\t‚Ä¢\tor centroids too far from any Gaussian,\n",
    "\t‚Ä¢\tor even centroids located between Gaussians rather than inside them.\n",
    "\n",
    "For example:\n",
    "If we initialize a centroid exactly in the middle of the plane, equidistant from all points, the assignments may be completely wrong, and the algorithm may fail to classify the data at all.\n",
    "\n",
    "Because of bad initialization, a centroid may be placed exactly in a location where it cannot capture any meaningful cluster structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10:\n",
    "\n",
    "What can you say about those configurations of centroids? What does it imply concerning the minima? Conclude your arguments by discussing the convexity of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Question 10**\n",
    "\n",
    "For a fixed assignment of points to clusters C_k, the cost function is:\n",
    "\n",
    "$J_k(\\mu_k)= \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2$.\n",
    "\n",
    "This function is convex in \\mu_k because:\n",
    "\t‚Ä¢\ta squared norm is a convex function,\n",
    "\t‚Ä¢\tthe sum of convex functions remains convex.\n",
    "\n",
    "For fixed assignments, the optimization is convex, and the unique minimizer is:\n",
    "\n",
    "$\\mu_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i$.\n",
    "\n",
    "However, when we change both the assignments $C_k$ and the centroids \\mu_k,\n",
    "\n",
    "$J = \\sum_{k=1}^K \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2$,\n",
    "\n",
    "the problem has K minima, and not all of them are global.\n",
    "Depending on the initialization, the problem may fall into a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11:\n",
    "\n",
    "We can also quickly generalize our algorithm.\n",
    "\n",
    "In some situation, you are aiming at favoring some directions in your data and penalizing the others, so that you can weigh the euclidean distance according to:\n",
    "\n",
    "$$d^{(w)}(x_{i}, \\mu(i)) = \\frac{\\sum_{j=1}^d w_i(x_{ij} - \\mu(i)_j)^2}{\\sum_{j=1}^d w_j} $$\n",
    "\n",
    "Show that with a change of variables, the problem remains the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11 ‚Äî Solution** \n",
    "\n",
    "We consider the weighted distance:\n",
    "\n",
    "\n",
    "\n",
    "$d^{(w)}(x_i ,\\mu(i))=\\frac{\\sum_{j=1}^d w_j (x_{ij}-\\mu(i)_j)^2}{\\sum_{j=1}^d w_j}$.\n",
    "\n",
    "Since $\\sum_{j} w_j$ is a constant independent of cluster assignments or centroids, minimizing\n",
    "$d^{(w)}$ is equivalent to minimizing the numerator:\n",
    "\n",
    "$\\sum_{j=1}^d w_j (x_{ij}-\\mu(i)_j)^2$.\n",
    "\n",
    "Summing over all points, the weighted K-means objective becomes:\n",
    "\n",
    "$J_w = \\sum_i \\sum_{j=1}^d w_j (x_{ij}-\\mu(i)_j)^2$.\n",
    "\n",
    "\n",
    "\n",
    "Change of Variables\n",
    "\n",
    "Define the transformed data and centroids:\n",
    "\n",
    "$y_{ij} = \\sqrt{w_j}\\, x_{ij},\n",
    "\\qquad\n",
    "\\nu_{kj} = \\sqrt{w_j}\\, \\mu_{kj}.$\n",
    "\n",
    "This rescaling simply stretches or shrinks each coordinate according to its weight.\n",
    "\n",
    "Now compute the Euclidean distance in the transformed space:\n",
    "\n",
    "$\\|y_i - \\nu_k\\|^2= \\sum_{j=1}^d (y_{ij}-\\nu_{kj})^2$.\n",
    "\n",
    "Substituting the definitions:\n",
    "\n",
    "$(y_{ij}-\\nu_{kj})^2= \\left(\\sqrt{w_j}\\,x_{ij} - \\sqrt{w_j}\\,\\mu_{kj}\\right)^2= w_j (x_{ij}-\\mu_{kj})^2$.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$\\|y_i - \\nu_k\\|^2= \\sum_{j=1}^d w_j (x_{ij}-\\mu_{kj})^2$.\n",
    "\n",
    "This is exactly the weighted K-means objective (up to a constant factor).\n",
    "\n",
    "\n",
    "After the change of variables:\n",
    "\n",
    "$y_{ij} = \\sqrt{w_j} \\, x_{ij}$,\n",
    "\n",
    "the weighted K-means objective:\n",
    "\n",
    "$J_w = \\sum_i \\sum_{j=1}^d w_j (x_{ij}-\\mu_{kj})^2$\n",
    "\n",
    "becomes the standard K-means objective:\n",
    "\n",
    "$J = \\sum_i \\|y_i - \\nu_{k(i)}\\|^2$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "Weighted K-means is equivalent to standard K-means applied to rescaled data.\n",
    "\n",
    "The clustering assignments and centroid updates are unchanged‚Äîonly the coordinate system changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Restricted Boltzmann Machine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The Boltzmann Machine have been inspired by thermodynamic and statistical physics models, more precisely they are part of the Energy Models using the well known Boltzmann Distribution as written in physics style:\n",
    "\n",
    "$$ P\\left( E \\right)  = \\frac{1}{Z} \\exp \\left( -\\frac{E}{k_b T} \\right)$$\n",
    "\n",
    "It becomes in statistical inference framework:\n",
    "$$\n",
    "P(\\mathbf{v} | J, \\mathbf{b}) \\propto e^{\\mathbf{v}^TJ\\mathbf{v} + \\mathbf{b}^T\\mathbf{v}} = e^{-E(\\mathbf{v})}\n",
    "$$\n",
    "where:\n",
    "- $\\mathbf{v}\\in\\mathbb{R}^n:$ The binary vector with components $v_i = 0 \\; {\\rm or} \\; 1$\n",
    "\n",
    "- $J \\in \\mathbb{R}^{n \\times n}:$ The coupling matrix\n",
    "\n",
    "- $\\mathbf{b} \\in  \\mathbb{R}^n$: Field\n",
    "\n",
    "- $E(\\mathbf{v}) \\in  \\mathbb{R}$: Energy\n",
    "\n",
    "\n",
    "However, one problem arised with initial Boltzmann Machine (BM) -- like its parent models in statistical physics (as the SK model) -- all the units are interacting through complicated dependencies. For example, if we consider 3 components of $\\mathbf{v}$: $v_1$, $v_2$, and $v_3$, there are trivial interactions such as one modelised by $P(v_1, v_2)$ corresponding to the correlation between the two first components of $\\mathbf{v}$, but there are also none trivial interactions. Indeed, if some term like $P(v_1, v_2 | v_3)$ which suggests that the correlation $x_1$ and $v_2$ depends on $v_3$ and this is clearly none linear.\n",
    "\n",
    "A really ingenious way to overcome this situation is to replace all the tricky interactions between the units $\\mathbf{v}\\in\\mathbb{R}^n$ by connections through hidden units $\\mathbf{h}\\in\\mathbb{R}^m$, artifically created. Indeed, correlations between two units $v_1$ and $v_2$ (specially the dependency of their correlations on other units $v_3$, $v_4$,...) can be atrificially replaced by introducing a third unit $h_1$ and considerin only linear correlations between $v_1 \\leftrightarrow h_1$, $h_1 \\leftrightarrow v_2$ and $v_1 \\leftrightarrow v_2$. The units $v_i$ are now called the visible units. This model is the most known version of BMs. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"boltzmannmachine.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "However, this model is still fully connected and makes the computation really costful. Then, one can even simplify the model by considering zero intra layer interractions. This simplified model is call Restricted Boltzmann Machine (RBM) (Physics Nobel Price 2024 ü•≥).\n",
    "\n",
    "Thus, the RBM architecture consists of two layers of binary stochastic units: a $\\textbf{visible layer}$ $\\mathbf{v}$ and a $\\textbf{hidden layer}$ $\\mathbf{h}$. The layers are fully connected, but there are no connections within a layer, making the model a $\\textbf{bipartite graph}$. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"rbm.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) are a class of energy-based probabilistic graphical models that are commonly used in machine learning for tasks such as dimensionality reduction, feature learning, and generative modeling.\n",
    "\n",
    "### Energy Function and Probabilities\n",
    "\n",
    "The joint configuration of the visible units $\\mathbf{v} \\in \\{0, 1\\}^d$ and the hidden units $\\mathbf{h} \\in \\{0, 1\\}^m$ is associated with an $\\textbf{energy function}$, defined as:\n",
    "\n",
    "$$ E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^\\top \\mathbf{W} \\mathbf{h} - \\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h}$$\n",
    "where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$ is the weight matrix connecting the visible and hidden units,\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^d$ field of the visible units or also called the biases of the visible units,\n",
    "- $\\mathbf{c} \\in \\mathbb{R}^m$ field of the hidden units of also called the biases of the hidden units.\n",
    "\n",
    "The energy function determines the joint probability distribution over $\\mathbf{v}$ and $\\mathbf{h}$:\n",
    "$$ P(\\mathbf{v}, \\mathbf{h}) = \\frac{1}{Z} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "where $Z$ is the $\\textbf{partition function}$, ensuring normalization:\n",
    "\n",
    "$$ Z = \\sum_{\\mathbf{v}, \\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "\n",
    "\n",
    "The marginal probability of the visible units $\\mathbf{v}$ is obtained by summing over all possible configurations of the hidden units:\n",
    "\n",
    "$$ P(\\mathbf{v}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12:\n",
    "\n",
    "Write a valid expression of the energy $E(\\textbf{v}, \\textbf{h})$ in the case of a BM (non-restricted) with an hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12 - Answer**\n",
    "\n",
    "In the case of a (non-restricted) Boltzmann Machine with a hidden layer, we know that we have dependencies between variables. In an RBM, we transform this with a restricted (bipartite) structure.\n",
    "\n",
    "A generic (non-restricted) Boltzmann machine over visibles v can be written as:\n",
    "$E(v;J,b) = -v^\\top J v - b^\\top v$.\n",
    "\n",
    "For an RBM with visible units v and hidden units h, the energy is:\n",
    "$E(v,h;W,b,c) = -v^\\top W h - b^\\top v - c^\\top h$.\n",
    "We found this expression by identifying it with the general energy form and restricting connections to be only between v and h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13:\n",
    "\n",
    "One of the key properties of RBMs is the $\\textbf{conditional independence}$ between units within a layer:\n",
    "\n",
    "Compute the conditional probability and show that:\n",
    "\n",
    "$$ P(h_j = 1 | \\mathbf{v}) = \\sigma\\left(c_j + \\sum_{i} v_i W_{ij}\\right) $$\n",
    "and\n",
    "$$ P(v_i = 1 | \\mathbf{h}) = \\sigma\\left(b_i + \\sum_{j} h_j W_{ij}\\right) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid activation function.\n",
    "\n",
    "This bipartite structure enables efficient Gibbs sampling for approximating the intractable joint distribution $P(\\mathbf{v}, \\mathbf{h})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13 - Answer**\n",
    "\n",
    "Here we want to explore the conditional independence. We know that\n",
    "\n",
    "$P(v,h)=\\frac{1}{Z}\\exp(-E(v,h)), \\qquad P(v)=\\frac{1}{Z}\\sum_h \\exp(-E(v,h))$.\n",
    "\n",
    "The RBM energy is\n",
    "\n",
    "$E(v,h)= -\\sum_i b_i v_i \\;-\\;\\sum_j c_j h_j \\;-\\;\\sum_{i,j} v_i W_{ij} h_j$,\n",
    "so\n",
    "$-E(v,h)= \\sum_i b_i v_i + \\sum_j c_j h_j + \\sum_{i,j} v_i W_{ij} h_j$.\n",
    "\n",
    "By definition of conditional probability,\n",
    "\n",
    "$P(h_j=1\\mid v)=\\frac{P(h_j=1,v)}{P(v)}=\\frac{\\sum_{h_{k\\neq j}}P(v,h_j=1,h_{k\\neq j})}{\\sum_{h_{k\\neq j}}P(v,h_j=0,h_{k\\neq j})+\\sum_{h_{k\\neq j}}P(v,h_j=1,h_{k\\neq j})}$.\n",
    "\n",
    "Using $P(v,h)\\propto \\exp(-E(v,h))$, the factor $\\frac{1}{Z}$ cancels in the ratio and we get\n",
    "\n",
    "$P(h_j=1\\mid v)=\\frac{\\sum_{h_{k\\neq j}}\\exp\\big(-E(v,h_j=1,h_{k\\neq j})\\big)}{\\sum_{h_{k\\neq j}}\\exp\\big(-E(v,h_j=0,h_{k\\neq j})\\big)+\\sum_{h_{k\\neq j}}\\exp\\big(-E(v,h_j=1,h_{k\\neq j})\\big)}$.\n",
    "\n",
    "Now isolate the part of $-E(v,h)$ that depends on $h_j$. We can write\n",
    "\n",
    "$-E(v,h)=\\Big(\\sum_i b_i v_i\\Big)+\\Big(\\sum_{k\\neq j} c_k h_k + \\sum_{i,k\\neq j} v_i W_{ik}h_k\\Big)+h_j\\Big(c_j+\\sum_i v_i W_{ij}\\Big)$.\n",
    "\n",
    "Define\n",
    "\n",
    "$A(v,h_{k\\neq j})=\\sum_i b_i v_i+\\sum_{k\\neq j} c_k h_k+\\sum_{i,k\\neq j} v_i W_{ik}h_k,\\qquad\\alpha_j(v)=c_j+\\sum_i v_i W_{ij}$.\n",
    "\n",
    "Then\n",
    "\n",
    "$\\exp(-E(v,h))=\\exp\\big(A(v,h_{k\\neq j})\\big)\\exp\\big(h_j\\,\\alpha_j(v)\\big)$.\n",
    "\n",
    "So when $h_j=0$,\n",
    "\n",
    "$\\exp(-E(v,h_j=0,h_{k\\neq j}))=\\exp(A(v,h_{k\\neq j}))$,\n",
    "\n",
    "and when $h_j=1$,\n",
    "\n",
    "$\\exp(-E(v,h_j=1,h_{k\\neq j}))=\\exp(A(v,h_{k\\neq j}))\\,\\exp(\\alpha_j(v))$.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$\\sum_{h_{k\\neq j}}\\exp(-E(v,h_j=0,h_{k\\neq j}))=\\sum_{h_{k\\neq j}}\\exp(A(v,h_{k\\neq j}))\\equiv S$,\n",
    "\n",
    "and\n",
    "\n",
    "$\\sum_{h_{k\\neq j}}\\exp(-E(v,h_j=1,h_{k\\neq j}))=\\exp(\\alpha_j(v))\\sum_{h_{k\\neq j}}\\exp(A(v,h_{k\\neq j}))=\\exp(\\alpha_j(v))\\,S$.\n",
    "\n",
    "Plugging into the conditional probability ratio gives\n",
    "\n",
    "$P(h_j=1\\mid v)=\\frac{\\exp(\\alpha_j(v))\\,S}{S+\\exp(\\alpha_j(v))\\,S}=\\frac{\\exp(\\alpha_j(v))}{1+\\exp(\\alpha_j(v))}=\\sigma\\big(\\alpha_j(v)\\big)$.\n",
    "\n",
    "Hence\n",
    "\n",
    "$P(h_j=1\\mid v)=\\sigma\\!\\left(c_j+\\sum_i v_i W_{ij}\\right)$.\n",
    "\n",
    "Now do the same for $P(v_i=1\\mid h)$. By definition,\n",
    "\n",
    "$P(v_i=1\\mid h)=\\frac{P(v_i=1,h)}{\\sum_{v_i\\in\\{0,1\\}}P(v_i,h)}=\\frac{\\sum_{v_{k\\neq i}}\\exp\\big(-E(v_i=1,v_{k\\neq i},h)\\big)}{\\sum_{v_{k\\neq i}}\\exp\\big(-E(v_i=0,v_{k\\neq i},h)\\big)+\\sum_{v_{k\\neq i}}\\exp\\big(-E(v_i=1,v_{k\\neq i},h)\\big)}.$\n",
    "\n",
    "We isolate the part of -E(v,h) that depends on v_i. Write\n",
    "\n",
    "$-E(v,h)=\\Big(\\sum_{k\\neq i} b_k v_k\\Big)+\\Big(\\sum_j c_j h_j + \\sum_{k\\neq i,j} v_k W_{kj}h_j\\Big)+v_i\\Big(b_i+\\sum_j W_{ij}h_j\\Big)$.\n",
    "\n",
    "Define\n",
    "\n",
    "$B(h,v_{k\\neq i})=\\sum_{k\\neq i} b_k v_k+\\sum_j c_j h_j+\\sum_{k\\neq i,j} v_k W_{kj}h_j,\\qquad\\beta_i(h)=b_i+\\sum_j W_{ij}h_j$.\n",
    "\n",
    "Then\n",
    "\n",
    "$\\exp(-E(v,h))=\\exp\\big(B(h,v_{k\\neq i})\\big)\\exp\\big(v_i\\,\\beta_i(h)\\big)$.\n",
    "\n",
    "So when v_i=0,\n",
    "$\\exp(-E(v_i=0,v_{k\\neq i},h))=\\exp(B(h,v_{k\\neq i}))$,\n",
    "and when $v_i=$1$,\n",
    "$\\exp(-E(v_i=1,v_{k\\neq i},h))=\\exp(B(h,v_{k\\neq i}))\\,\\exp(\\beta_i(h))$.\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$\\sum_{v_{k\\neq i}}\\exp(-E(v_i=0,v_{k\\neq i},h))=\\sum_{v_{k\\neq i}}\\exp(B(h,v_{k\\neq i}))\\equiv T$,\n",
    "and\n",
    "\n",
    "$\\sum_{v_{k\\neq i}}\\exp(-E(v_i=1,v_{k\\neq i},h))=\\exp(\\beta_i(h))\\sum_{v_{k\\neq i}}\\exp(B(h,v_{k\\neq i}))=\\exp(\\beta_i(h))\\,T$.\n",
    "\n",
    "Plugging into the ratio gives\n",
    "\n",
    "$P(v_i=1\\mid h)=\\frac{\\exp(\\beta_i(h))\\,T}{T+\\exp(\\beta_i(h))\\,T}=\\frac{\\exp(\\beta_i(h))}{1+\\exp(\\beta_i(h))}=\\sigma\\big(\\beta_i(h)\\big)$.\n",
    "\n",
    "Hence\n",
    "\n",
    "$P(v_i=1\\mid h)=\\sigma\\!\\left(b_i+\\sum_j W_{ij}h_j\\right)$.\n",
    "\n",
    "As a consequence, because each $h_j$ depends on v only through $\\alpha_j(v)$ and appears separated in the exponent, the conditional distribution factorizes:\n",
    "\n",
    "$P(h\\mid v)=\\prod_{j} P(h_j\\mid v)$,$\\qquad P(v\\mid h)=\\prod_i P(v_i\\mid h)$,\n",
    "\n",
    "which is the conditional independence property within each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in RBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14:\n",
    "\n",
    "Training an RBM involves maximizing the likelihood of the data distribution. To do so we are aiming at using a gradient descent/ascent on the weights (and biases).\n",
    "\n",
    "Compute the log-likelihood $\\mathcal{L}(\\mathbf{v})$, remember that the model is part of the unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the RBM is generative, the likelihood of parameters (W,b,c) is defined with respect to visible data v. The marginal likelihood is:\n",
    "$P(v)=\\frac{1}{Z}\\sum_h e^{-E(v,h)}$.\n",
    "\n",
    "For a dataset $\\{v^{(n)}\\}_{n=1}^N $(i.i.d.), the log-likelihood is:\n",
    "$\\log L(W,b,c)=\\sum_{n=1}^N \\log P(v^{(n)})$.\n",
    "\n",
    "Using the RBM structure (binary hidden units), for one v:\n",
    "$\\log P(v)= b^\\top v+\\sum_{j=1}^{H}\\log\\Big(1+\\exp\\big(c_j+\\sum_{i=1}^{D} v_i W_{ij}\\big)\\Big)-\\log Z$.\n",
    "\n",
    "And the partition function:\n",
    "$Z=\\sum_{v\\in\\{0,1\\}^D}\\sum_{h\\in\\{0,1\\}^H} e^{-E(v,h)}=\\sum_{v\\in\\{0,1\\}^D}\\exp(b^\\top v)\\prod_{j=1}^{H}\\Big(1+\\exp\\big(c_j+\\sum_{i=1}^{D} v_i W_{ij}\\big)\\Big)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15:\n",
    "\n",
    "Compute the gradient of the log-likelihood with respect to the weights $\\mathbf{W}$ and the biases $\\mathbf{b}$, $\\mathbf{c}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one training vector v, the key identity is:\n",
    "\n",
    "$\\frac{\\partial \\log P(v)}{\\partial \\theta}=-\\mathbb{E}_{p(h\\mid v)}\\left[\\frac{\\partial E(v,h)}{\\partial \\theta}\\right]+\\mathbb{E}_{p(v,h)}\\left[\\frac{\\partial E(v,h)}{\\partial \\theta}\\right]$.\n",
    "\n",
    "From:\n",
    "\n",
    "$E(v,h)= -\\sum_{i,j} v_i W_{ij} h_j - \\sum_i b_i v_i - \\sum_j c_j h_j$,\n",
    "\n",
    "we have:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial W_{ij}}=-v_i h_j, \\qquad \\frac{\\partial E}{\\partial b_i}=-v_i,\\qquad\\frac{\\partial E}{\\partial c_j}=-h_j$.\n",
    "\n",
    "Plugging into the identity gives:\n",
    "\n",
    "$\\frac{\\partial \\log P(v)}{\\partial W_{ij}}=\\mathbb{E}_{p(h\\mid v)}[v_i h_j]-\\mathbb{E}_{p(v,h)}[v_i h_j]$,\n",
    "\n",
    "$\\frac{\\partial \\log P(v)}{\\partial b_i}=\\mathbb{E}_{p(h\\mid v)}[v_i]-\\mathbb{E}_{p(v,h)}[v_i]$,\n",
    "\n",
    "$\\frac{\\partial \\log P(v)}{\\partial c_j}=\\mathbb{E}_{p(h\\mid v)}[h_j]-\\mathbb{E}_{p(v,h)}[h_j].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it should be possible to implement the RBM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16: (Open question)\n",
    "\n",
    "While it seems possible to run RBM algorithm, note that the second term in the gradient w.r.t. $\\mathbf{W}$ is computationally expensive due to the intractability of $Z$, the approximation Contrastive Divergence - k is often use. Research what is this approximation, is this approximation enough, why? Explain it with your own words and cite the papers you used for your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrastive Divergence (CD) is an approximation used to train an RBM when the true maximum-likelihood gradient is too expensive.\n",
    "\n",
    "The gradient has the form (for W):\n",
    "$\\frac{\\partial \\log p(v)}{\\partial W}=\\mathbb{E}_{p(h\\mid v)}[v h^\\top]-\\mathbb{E}_{p(v,h)}[v h^\\top]$.\n",
    "\n",
    "The first expectation is easy to compute because $p(h\\mid v)$ factorizes (sigmoid units). The second expectation is difficult because it requires samples from the model equilibrium distribution $p(v,h)$, which usually needs a long MCMC chain.\n",
    "\n",
    "Maximum likelihood would need the negative-phase expectation under the true model distribution $p(v,h)$ (equilibrium). CD replaces it by an approximation obtained after k Gibbs steps starting at the data. So it is not exactly the true likelihood gradient.\n",
    "\n",
    "Early in training, the model‚Äôs ‚Äúbad beliefs‚Äù are close to the data (starting the chain at the data makes sense) and CD gives a low-variance, cheap learning signal compared to running very long chains.\n",
    "\n",
    "If the model distribution is far from the data or has many separated modes, a few Gibbs steps may not move enough, making the negative phase biased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of RBMs\n",
    "\n",
    "RBMs are widely used in tasks such as:\n",
    "\n",
    "- $\\textbf{Dimensionality reduction}$: Similar to PCA but capable of capturing non-linear structures,\n",
    "- $\\textbf{Feature learning}$: For pre-training deep neural networks,\n",
    "- $\\textbf{Collaborative filtering}$: Used in recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
