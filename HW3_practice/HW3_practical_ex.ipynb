{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtmyHbT8R_-h"
      },
      "source": [
        "# Homework 3: optimization of a CNN model\n",
        "The task of this homework is to optimize a CNN model for the CIFAR-100. You are free to define the architecture of the model, and the training procedure. The only contraints are:\n",
        "- It must be a `torch.nn.Module` object\n",
        "- The number of trained parameters must be less than 1 million\n",
        "- The test dataset must not be used for any step of training.\n",
        "- The final training notebook should run on Google Colab within a maximum 1 hour approximately.\n",
        "- Do not modify the random seed, as they are needed for reproducibility purpose.\n",
        "\n",
        "For the grading, you must use the `evaluate` function defined below. It takes a model as input, and returns the test accuracy as output.\n",
        "\n",
        "As a guideline, you are expected to **discuss** and motivate your choices regarding:\n",
        "- Model architecture\n",
        "- Hyperparameters (learning rate, batch size, etc)\n",
        "- Regularization methods\n",
        "- Optimizer\n",
        "- Validation scheme\n",
        "\n",
        "A code without any explanation of the choices will not be accepted. Test accuracy is not the only measure of success for this homework.\n",
        "\n",
        "Remember that most of the train process is randomized, store your model's weights after training and load it before the evaluation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R9ITfVvR_-h"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6LSnReuR_-h"
      },
      "source": [
        "### Loading packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "eiIddXbzR_-h",
        "outputId": "46691678-cb09-4a70-d9a4-0a1fb94cd397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Fix all random seeds\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# For full determinism\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Import the best device available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "# load the data\n",
        "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xUd96430R_-i"
      },
      "outputs": [],
      "source": [
        "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "def evaluate(model):\n",
        "    params_count = sum(p.numel() for p in model.parameters())\n",
        "    print('The model has {} parameters'.format(params_count))\n",
        "\n",
        "    if params_count > int(1e6):\n",
        "        print('The model has too many parameters! Not allowed to evaluate.')\n",
        "        return\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    # print in bold red in a notebook\n",
        "    print('\\033[1m\\033[91mAccuracy on the test set: {}%\\033[0m'.format(100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are not allowed to use the testset which is normal , we then define a training and validation set so we can evaluate properly the training set with the validation set."
      ],
      "metadata": {
        "id": "3qcQ1llrW2Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the train dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data/',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "# Split the dataset into 40k-10k samples for training-validation.\n",
        "from torch.utils.data import random_split\n",
        "train_dataset,  valid_dataset = random_split(\n",
        "    train_dataset,\n",
        "    lengths=[40000, 10000],\n",
        "    generator=torch.Generator().manual_seed(42) #use a generator to insure reproducibilty\n",
        ")\n",
        "\n",
        "# what is the type of the \"new\" training dataset?\n",
        "print(train_dataset)\n"
      ],
      "metadata": {
        "id": "p6StH8OVVdWt",
        "outputId": "b6d90ad1-9275-4988-febf-3e0a5ae57343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataset.Subset object at 0x7b5cafbef500>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Tuple, List\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def fit(\n",
        "    model: nn.Module,\n",
        "    train_dataloader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epochs: int,\n",
        "    device: torch.device,\n",
        "    valid_dataloader: Optional[DataLoader] = None,\n",
        "    criterion: Optional[nn.Module] = None,\n",
        ") -> Tuple[List[float], List[float], List[float]]:\n",
        "    \"\"\"\n",
        "    Calls train_epoch() for a specified number of epochs, and (optionally) evaluates on valid_dataloader.\n",
        "\n",
        "    Modified so it also does the classic training-loop steps:\n",
        "    - moves inputs/targets to device\n",
        "    - forward pass\n",
        "    - compute loss\n",
        "    - backward\n",
        "    - optimizer step\n",
        "\n",
        "    If your existing train_epoch already does this, you can keep using it by passing\n",
        "    a train_epoch that accepts `criterion`, or remove the inlined loop below.\n",
        "    \"\"\"\n",
        "\n",
        "    if criterion is None:\n",
        "        # default for classification; change if your task is different\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses: List[float] = []\n",
        "    valid_losses: List[float] = []\n",
        "    valid_accs: List[float] = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---- TRAIN (inlined training loop like your screenshot) ----\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        num_samples = 0\n",
        "\n",
        "        for images, labels in train_dataloader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = images.size(0)\n",
        "            running_loss += loss.item() * bs\n",
        "            num_samples += bs\n",
        "\n",
        "        train_loss = running_loss / max(1, num_samples)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # ---- VALIDATION  ----\n",
        "        if valid_dataloader is not None:\n",
        "            valid_loss, valid_acc = predict(model, valid_dataloader, device, verbose=False)\n",
        "            valid_losses.append(valid_loss)\n",
        "            valid_accs.append(valid_acc)\n",
        "            print(\n",
        "                f\"Epoch {epoch}: Train Loss={train_loss:.4f}, \"\n",
        "                f\"Validation Loss={valid_loss:.4f}, Validation acc={valid_acc:.4f}\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}\")\n",
        "\n",
        "    return train_losses, valid_losses, valid_accs"
      ],
      "metadata": {
        "id": "z_IqhD3ztSNf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "def plot_losses(\n",
        "    train_losses: List[float],\n",
        "    valid_losses: Optional[List[float]] = None,\n",
        "    title: str = \"Loss per epoch\",\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plots training loss (and optional validation loss) vs epoch.\n",
        "\n",
        "    Usage:\n",
        "        train_losses, valid_losses, valid_accs = fit(...)\n",
        "        plot_losses(train_losses, valid_losses)\n",
        "    \"\"\"\n",
        "    epochs = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, train_losses, label=\"train loss\")\n",
        "\n",
        "    if valid_losses is not None and len(valid_losses) > 0:\n",
        "        # If valid_losses length differs (shouldn't, but safe), align to its length\n",
        "        v_epochs = list(range(1, len(valid_losses) + 1))\n",
        "        plt.plot(v_epochs, valid_losses, label=\"valid loss\")\n",
        "\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "b3JfQQeftY4y"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    verbose: bool = True,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluates a classification model and returns:\n",
        "      - average loss over all samples\n",
        "      - accuracy over all samples\n",
        "\n",
        "    Assumes dataloader yields (inputs, labels) and model outputs logits of shape (B, num_classes).\n",
        "    Uses CrossEntropyLoss by default (standard for CIFAR-100 classification).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")  # sum to average properly over samples\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        outputs = model(images)  # logits\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_samples)\n",
        "    acc = total_correct / max(1, total_samples)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Loss={avg_loss:.4f}, Accuracy={acc:.4f}\")\n",
        "\n",
        "    return avg_loss, acc"
      ],
      "metadata": {
        "id": "iSsFoeKjvDic"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture Used  : ResNet\n",
        "\n",
        "In the lab, we used a ResNet as the final model for CIFAR-10, so we will start from the same baseline here. However, the ResNet used for CIFAR-10 has over 1 million parameters, so we decided to change a bit the architecture first . In fact , the model begins with 64 channels and then increases the width at each stage to 128, 256, and 512 channels. With two 3×3 convolutions per block, most of the parameters come from convolution layers, and the number of convolution parameters scales as $3\\cdot 3\\cdot C_{in}\\cdot C_{out}$. As a result, doubling the number of channels roughly multiplies the parameter count by about four, meaning the later stages (especially the 256→512 part) dominate the total number of parameters.\n",
        "\n",
        "Since our goal is to train on CIFAR-100 while keeping the model efficient, we decided to reduce the parameter count rather than increasing model capacity further. CIFAR-100 images are only 32×32, and after several downsampling steps the spatial resolution becomes very small (e.g., 8×8 then 4×4). At that point, having 512 channels can be unnecessarily expensive: it increases computation and memory usage and can also increase the risk of overfitting without guaranteeing better generalization.\n",
        "\n",
        "Therefore, we redesigned the network by reducing the width (for example starting at 32 instead of 64) and/or capping the maximum number of channels (for example 256 instead of 512). This directly reduces parameters because it lowers $C_{in}$ and $C_{out}$ in every convolution, giving a large reduction with a minimal change to the overall architecture. Importantly, we keep the same global structure (stacked convolutional blocks with occasional downsampling and global average pooling) so that the model preserves the same inductive bias that works well on CIFAR-style datasets, while being more efficient and better matched to the dataset and compute constraints.\n"
      ],
      "metadata": {
        "id": "roqVWuhfqVKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters : Batch Size , Learning Rate and Momemtum\n",
        "\n",
        "We will focus now  on improving performance by tuning the training setup: the optimizer, the learning rate (and its schedule), the batch size, and related hyperparameters.\n",
        "\n",
        "The choice of optimizer and its hyperparameters plays a crucial role in final accuracy and training stability. Therefore, we will run cross-validation directly on the ResNet to select these settings properly. The motivation is that we want training to progress quickly while still converging to a good solution. With a learning rate that is too small, ResNet training is usually stable but slow; with a learning rate that is too large, updates can become unstable, the loss can oscillate, parameters may overshoot good regions, and convergence can fail. Large updates can also amplify exploding or vanishing gradient effects through the repeated gradient multiplications of backpropagation, which can prevent early layers from learning effectively.\n",
        "\n",
        "The goal is to achieve the best trade-off: fast learning without sacrificing stability, and this is exactly what careful hyperparameter selection (optimizer, learning rate, batch size, and scheduling) is meant to provide.\n"
      ],
      "metadata": {
        "id": "5oTCJWQ59-2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aHrqz-Ih9ZRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NonResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels = in_planes,\n",
        "            out_channels = planes,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            planes,\n",
        "            planes,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100, base=32, max_planes=128):\n",
        "        super().__init__()\n",
        "        self.in_planes = base\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, base, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(base)\n",
        "\n",
        "        # widths: base, 2*base, 4*base, 8*base but capped\n",
        "        widths = [base, min(2*base, max_planes), min(4*base, max_planes), min(8*base, max_planes)]\n",
        "\n",
        "        self.layer1 = self._make_layer(block, widths[0], num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, widths[1], num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, widths[2], num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, widths[3], num_blocks[3], stride=2)\n",
        "\n",
        "        self.fc = nn.Linear(widths[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for s in strides:\n",
        "            layers.append(block(self.in_planes, planes, s))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = F.adaptive_avg_pool2d(out, 1)   # (B, C, 1, 1)\n",
        "        out = torch.flatten(out, 1)           # (B, C)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "print(\"Model parameters: \", sum(p.numel() for p in ResNet(block=NonResidualBlock, num_blocks=[1,1,1,1]).parameters()))\n"
      ],
      "metadata": {
        "id": "rl72HHAcriOG",
        "outputId": "9b0497f0-2eea-4998-dce8-3f81cc3731d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters:  605060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grid of Hyperparameters\n",
        "\n",
        "learning_rates = [0.3,0.2,0.1]\n",
        "batch_sizes =  [128, 256,]\n",
        "momentums = [0.7,0.9]"
      ],
      "metadata": {
        "id": "Ier7ak6Prp80"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's run cross validation on the three components :\n",
        "for l_r in learning_rates:\n",
        "    for b_s in batch_sizes:\n",
        "      for mt in momentums:\n",
        "\n",
        "        # define the hyperparameters\n",
        "        BATCH_SIZE = b_s\n",
        "        TEST_BATCH_SIZE = 1024\n",
        "        LEARNING_RATE = l_r\n",
        "\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            dataset=train_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            num_workers=2)\n",
        "\n",
        "        valid_dataloader = DataLoader(\n",
        "            dataset=valid_dataset,\n",
        "            batch_size=TEST_BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=2)\n",
        "\n",
        "\n",
        "        print(f\"Training with lr={l_r}, batch_size={b_s} and , momentum = {mt} : \")\n",
        "\n",
        "        # initialize model\n",
        "        resnet = ResNet(block=NonResidualBlock, num_blocks=[1,1,1,1]).to(device)\n",
        "        # define the optimizer\n",
        "        optimizer = torch.optim.SGD(resnet.parameters(), lr=l_r, momentum=mt)\n",
        "\n",
        "        # train the CNN\n",
        "        losses = fit(\n",
        "             model=resnet,\n",
        "             train_dataloader=train_dataloader,\n",
        "             valid_dataloader=valid_dataloader,\n",
        "             optimizer=optimizer,\n",
        "             epochs=10,\n",
        "             device=device\n",
        "             )\n",
        "\n",
        "\n",
        "        print(\"***********************************************************\")\n",
        "        print(\"***********************************************************\")\n",
        "        print(\"***********************************************************\")\n"
      ],
      "metadata": {
        "id": "GEnijHWUsfyG",
        "outputId": "da2cdf2a-e4e7-4959-ec1d-b50f5bfce342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with lr=0.3, batch_size=128 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=3.8815, Validation Loss=4.2020, Validation acc=0.0729\n",
            "Epoch 1: Train Loss=3.2550, Validation Loss=3.2549, Validation acc=0.2158\n",
            "Epoch 2: Train Loss=2.7763, Validation Loss=2.9361, Validation acc=0.2585\n",
            "Epoch 3: Train Loss=2.3864, Validation Loss=2.6447, Validation acc=0.3115\n",
            "Epoch 4: Train Loss=2.0695, Validation Loss=2.5380, Validation acc=0.3546\n",
            "Epoch 5: Train Loss=1.8101, Validation Loss=2.4284, Validation acc=0.3820\n",
            "Epoch 6: Train Loss=1.5816, Validation Loss=2.2264, Validation acc=0.4270\n",
            "Epoch 7: Train Loss=1.3752, Validation Loss=2.3797, Validation acc=0.4004\n",
            "Epoch 8: Train Loss=1.2032, Validation Loss=2.3500, Validation acc=0.4397\n",
            "Epoch 9: Train Loss=1.0215, Validation Loss=2.4917, Validation acc=0.4173\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=128 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=3.8975, Validation Loss=3.9836, Validation acc=0.0980\n",
            "Epoch 1: Train Loss=3.2537, Validation Loss=3.1555, Validation acc=0.2174\n",
            "Epoch 2: Train Loss=2.7365, Validation Loss=2.8071, Validation acc=0.2816\n",
            "Epoch 3: Train Loss=2.3350, Validation Loss=3.1197, Validation acc=0.2525\n",
            "Epoch 4: Train Loss=2.0336, Validation Loss=2.4347, Validation acc=0.3706\n",
            "Epoch 5: Train Loss=1.7766, Validation Loss=2.1847, Validation acc=0.4240\n",
            "Epoch 6: Train Loss=1.5563, Validation Loss=2.3689, Validation acc=0.4057\n",
            "Epoch 7: Train Loss=1.3579, Validation Loss=2.1888, Validation acc=0.4435\n",
            "Epoch 8: Train Loss=1.1700, Validation Loss=2.2365, Validation acc=0.4545\n",
            "Epoch 9: Train Loss=1.0016, Validation Loss=2.3568, Validation acc=0.4522\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=128 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=3.9844, Validation Loss=3.7671, Validation acc=0.1096\n",
            "Epoch 1: Train Loss=3.4449, Validation Loss=3.3361, Validation acc=0.1899\n",
            "Epoch 2: Train Loss=3.0614, Validation Loss=2.9870, Validation acc=0.2551\n",
            "Epoch 3: Train Loss=2.6998, Validation Loss=2.8255, Validation acc=0.2843\n",
            "Epoch 4: Train Loss=2.3766, Validation Loss=2.5468, Validation acc=0.3417\n",
            "Epoch 5: Train Loss=2.1076, Validation Loss=2.3315, Validation acc=0.3865\n",
            "Epoch 6: Train Loss=1.8783, Validation Loss=2.3879, Validation acc=0.3960\n",
            "Epoch 7: Train Loss=1.6555, Validation Loss=2.3073, Validation acc=0.4166\n",
            "Epoch 8: Train Loss=1.4695, Validation Loss=2.2445, Validation acc=0.4420\n",
            "Epoch 9: Train Loss=1.2897, Validation Loss=2.3549, Validation acc=0.4304\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=256 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=3.9346, Validation Loss=4.4273, Validation acc=0.0709\n",
            "Epoch 1: Train Loss=3.3567, Validation Loss=3.6951, Validation acc=0.1452\n",
            "Epoch 2: Train Loss=2.9191, Validation Loss=3.7014, Validation acc=0.1529\n",
            "Epoch 3: Train Loss=2.5260, Validation Loss=3.2006, Validation acc=0.2389\n",
            "Epoch 4: Train Loss=2.1828, Validation Loss=3.2794, Validation acc=0.2677\n",
            "Epoch 5: Train Loss=1.9293, Validation Loss=2.8035, Validation acc=0.3255\n",
            "Epoch 6: Train Loss=1.7071, Validation Loss=3.9138, Validation acc=0.2136\n",
            "Epoch 7: Train Loss=1.5118, Validation Loss=2.8083, Validation acc=0.3513\n",
            "Epoch 8: Train Loss=1.3272, Validation Loss=2.5274, Validation acc=0.3888\n",
            "Epoch 9: Train Loss=1.1446, Validation Loss=3.0246, Validation acc=0.3526\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=256 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=3.8864, Validation Loss=3.9078, Validation acc=0.1157\n",
            "Epoch 1: Train Loss=3.2578, Validation Loss=4.0463, Validation acc=0.1289\n",
            "Epoch 2: Train Loss=2.7698, Validation Loss=3.2335, Validation acc=0.2149\n",
            "Epoch 3: Train Loss=2.3619, Validation Loss=2.8710, Validation acc=0.2811\n",
            "Epoch 4: Train Loss=2.0572, Validation Loss=2.6234, Validation acc=0.3353\n",
            "Epoch 5: Train Loss=1.8122, Validation Loss=2.5553, Validation acc=0.3530\n",
            "Epoch 6: Train Loss=1.5917, Validation Loss=2.2859, Validation acc=0.4086\n",
            "Epoch 7: Train Loss=1.3910, Validation Loss=2.5501, Validation acc=0.3772\n",
            "Epoch 8: Train Loss=1.1944, Validation Loss=2.6168, Validation acc=0.3989\n",
            "Epoch 9: Train Loss=1.0275, Validation Loss=2.7688, Validation acc=0.3893\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=256 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=3.9694, Validation Loss=3.7565, Validation acc=0.1088\n",
            "Epoch 1: Train Loss=3.4439, Validation Loss=3.5176, Validation acc=0.1525\n",
            "Epoch 2: Train Loss=3.0616, Validation Loss=3.1469, Validation acc=0.2215\n",
            "Epoch 3: Train Loss=2.6835, Validation Loss=3.0885, Validation acc=0.2521\n",
            "Epoch 4: Train Loss=2.3441, Validation Loss=2.6590, Validation acc=0.3235\n",
            "Epoch 5: Train Loss=2.0612, Validation Loss=2.3013, Validation acc=0.3830\n",
            "Epoch 6: Train Loss=1.8261, Validation Loss=2.2810, Validation acc=0.4035\n",
            "Epoch 7: Train Loss=1.6206, Validation Loss=2.3968, Validation acc=0.3894\n",
            "Epoch 8: Train Loss=1.4275, Validation Loss=2.1935, Validation acc=0.4443\n",
            "Epoch 9: Train Loss=1.2472, Validation Loss=2.4068, Validation acc=0.4162\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=512 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=4.0436, Validation Loss=4.0840, Validation acc=0.0663\n",
            "Epoch 1: Train Loss=3.5194, Validation Loss=4.8788, Validation acc=0.0497\n",
            "Epoch 2: Train Loss=3.1405, Validation Loss=4.5235, Validation acc=0.1042\n",
            "Epoch 3: Train Loss=2.7665, Validation Loss=3.8015, Validation acc=0.1394\n",
            "Epoch 4: Train Loss=2.4656, Validation Loss=5.1182, Validation acc=0.0864\n",
            "Epoch 5: Train Loss=2.1971, Validation Loss=5.5133, Validation acc=0.1127\n",
            "Epoch 6: Train Loss=1.9848, Validation Loss=3.2786, Validation acc=0.2356\n",
            "Epoch 7: Train Loss=1.7713, Validation Loss=4.4231, Validation acc=0.1636\n",
            "Epoch 8: Train Loss=1.5997, Validation Loss=4.1750, Validation acc=0.1710\n",
            "Epoch 9: Train Loss=1.4340, Validation Loss=3.3412, Validation acc=0.2408\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=512 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=3.9789, Validation Loss=4.9647, Validation acc=0.0366\n",
            "Epoch 1: Train Loss=3.4263, Validation Loss=4.0432, Validation acc=0.1033\n",
            "Epoch 2: Train Loss=3.0231, Validation Loss=5.2472, Validation acc=0.0918\n",
            "Epoch 3: Train Loss=2.6406, Validation Loss=4.6864, Validation acc=0.1129\n",
            "Epoch 4: Train Loss=2.3404, Validation Loss=3.2548, Validation acc=0.2343\n",
            "Epoch 5: Train Loss=2.0508, Validation Loss=2.8063, Validation acc=0.3009\n",
            "Epoch 6: Train Loss=1.8237, Validation Loss=3.4774, Validation acc=0.2284\n",
            "Epoch 7: Train Loss=1.6319, Validation Loss=2.9120, Validation acc=0.3352\n",
            "Epoch 8: Train Loss=1.4500, Validation Loss=3.3478, Validation acc=0.3041\n",
            "Epoch 9: Train Loss=1.2925, Validation Loss=2.6846, Validation acc=0.3626\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=512 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=4.0406, Validation Loss=4.5466, Validation acc=0.0596\n",
            "Epoch 1: Train Loss=3.5484, Validation Loss=3.6966, Validation acc=0.1417\n",
            "Epoch 2: Train Loss=3.1713, Validation Loss=3.8299, Validation acc=0.1530\n",
            "Epoch 3: Train Loss=2.8417, Validation Loss=3.1052, Validation acc=0.2404\n",
            "Epoch 4: Train Loss=2.5244, Validation Loss=2.8997, Validation acc=0.2921\n",
            "Epoch 5: Train Loss=2.2406, Validation Loss=3.0622, Validation acc=0.2698\n",
            "Epoch 6: Train Loss=1.9900, Validation Loss=2.4776, Validation acc=0.3561\n",
            "Epoch 7: Train Loss=1.7802, Validation Loss=2.4551, Validation acc=0.3762\n",
            "Epoch 8: Train Loss=1.5870, Validation Loss=2.4758, Validation acc=0.3981\n",
            "Epoch 9: Train Loss=1.4001, Validation Loss=2.6501, Validation acc=0.3744\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=1024 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=4.2159, Validation Loss=5.7474, Validation acc=0.0211\n",
            "Epoch 1: Train Loss=3.7648, Validation Loss=8.9371, Validation acc=0.0116\n",
            "Epoch 2: Train Loss=3.5456, Validation Loss=5.5822, Validation acc=0.0439\n",
            "Epoch 3: Train Loss=3.3029, Validation Loss=4.1646, Validation acc=0.0842\n",
            "Epoch 4: Train Loss=3.0793, Validation Loss=4.2320, Validation acc=0.1215\n",
            "Epoch 5: Train Loss=2.8353, Validation Loss=4.0367, Validation acc=0.1204\n",
            "Epoch 6: Train Loss=2.6216, Validation Loss=7.3389, Validation acc=0.0294\n",
            "Epoch 7: Train Loss=2.4819, Validation Loss=8.1514, Validation acc=0.0623\n",
            "Epoch 8: Train Loss=2.2983, Validation Loss=5.7102, Validation acc=0.0731\n",
            "Epoch 9: Train Loss=2.1118, Validation Loss=5.3557, Validation acc=0.1240\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=1024 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=4.1452, Validation Loss=4.6260, Validation acc=0.0332\n",
            "Epoch 1: Train Loss=3.6710, Validation Loss=5.7543, Validation acc=0.0394\n",
            "Epoch 2: Train Loss=3.4130, Validation Loss=6.5855, Validation acc=0.0364\n",
            "Epoch 3: Train Loss=3.1270, Validation Loss=4.4145, Validation acc=0.0793\n",
            "Epoch 4: Train Loss=2.9292, Validation Loss=4.7763, Validation acc=0.1003\n",
            "Epoch 5: Train Loss=2.6670, Validation Loss=3.9266, Validation acc=0.1628\n",
            "Epoch 6: Train Loss=2.4264, Validation Loss=9.0252, Validation acc=0.0444\n",
            "Epoch 7: Train Loss=2.2656, Validation Loss=4.3340, Validation acc=0.1587\n",
            "Epoch 8: Train Loss=2.0567, Validation Loss=4.3291, Validation acc=0.1746\n",
            "Epoch 9: Train Loss=1.9312, Validation Loss=4.0820, Validation acc=0.1993\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.3, batch_size=1024 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=4.1396, Validation Loss=5.6722, Validation acc=0.0484\n",
            "Epoch 1: Train Loss=3.6971, Validation Loss=3.9209, Validation acc=0.0986\n",
            "Epoch 2: Train Loss=3.4207, Validation Loss=3.8099, Validation acc=0.1248\n",
            "Epoch 3: Train Loss=3.1533, Validation Loss=4.6473, Validation acc=0.0825\n",
            "Epoch 4: Train Loss=2.9049, Validation Loss=3.2458, Validation acc=0.2097\n",
            "Epoch 5: Train Loss=2.6633, Validation Loss=3.8081, Validation acc=0.1931\n",
            "Epoch 6: Train Loss=2.4306, Validation Loss=2.7371, Validation acc=0.3037\n",
            "Epoch 7: Train Loss=2.2132, Validation Loss=2.7851, Validation acc=0.3060\n",
            "Epoch 8: Train Loss=2.0050, Validation Loss=3.4877, Validation acc=0.2358\n",
            "Epoch 9: Train Loss=1.8663, Validation Loss=2.6730, Validation acc=0.3441\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=128 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=3.8961, Validation Loss=3.7967, Validation acc=0.1066\n",
            "Epoch 1: Train Loss=3.2384, Validation Loss=3.3200, Validation acc=0.1896\n",
            "Epoch 2: Train Loss=2.7049, Validation Loss=3.4525, Validation acc=0.1926\n",
            "Epoch 3: Train Loss=2.3047, Validation Loss=2.8763, Validation acc=0.2752\n",
            "Epoch 4: Train Loss=2.0104, Validation Loss=2.4235, Validation acc=0.3752\n",
            "Epoch 5: Train Loss=1.7772, Validation Loss=2.2179, Validation acc=0.4160\n",
            "Epoch 6: Train Loss=1.5729, Validation Loss=2.3850, Validation acc=0.3898\n",
            "Epoch 7: Train Loss=1.3770, Validation Loss=2.6074, Validation acc=0.3845\n",
            "Epoch 8: Train Loss=1.2034, Validation Loss=2.5039, Validation acc=0.4096\n",
            "Epoch 9: Train Loss=1.0333, Validation Loss=2.2370, Validation acc=0.4426\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=128 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=3.8686, Validation Loss=4.8955, Validation acc=0.0573\n",
            "Epoch 1: Train Loss=3.2497, Validation Loss=3.4857, Validation acc=0.1709\n",
            "Epoch 2: Train Loss=2.7786, Validation Loss=3.0636, Validation acc=0.2376\n",
            "Epoch 3: Train Loss=2.3656, Validation Loss=2.6950, Validation acc=0.3165\n",
            "Epoch 4: Train Loss=2.0656, Validation Loss=2.4847, Validation acc=0.3637\n",
            "Epoch 5: Train Loss=1.8359, Validation Loss=2.3292, Validation acc=0.3940\n",
            "Epoch 6: Train Loss=1.6231, Validation Loss=2.3351, Validation acc=0.4008\n",
            "Epoch 7: Train Loss=1.4271, Validation Loss=2.2926, Validation acc=0.4197\n",
            "Epoch 8: Train Loss=1.2528, Validation Loss=2.2277, Validation acc=0.4443\n",
            "Epoch 9: Train Loss=1.0611, Validation Loss=2.3776, Validation acc=0.4270\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=128 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=3.9143, Validation Loss=3.9668, Validation acc=0.0963\n",
            "Epoch 1: Train Loss=3.3218, Validation Loss=3.3634, Validation acc=0.1848\n",
            "Epoch 2: Train Loss=2.8603, Validation Loss=3.0078, Validation acc=0.2421\n",
            "Epoch 3: Train Loss=2.4565, Validation Loss=2.7337, Validation acc=0.3125\n",
            "Epoch 4: Train Loss=2.1378, Validation Loss=2.3905, Validation acc=0.3825\n",
            "Epoch 5: Train Loss=1.8819, Validation Loss=2.2736, Validation acc=0.4037\n",
            "Epoch 6: Train Loss=1.6647, Validation Loss=2.1273, Validation acc=0.4309\n",
            "Epoch 7: Train Loss=1.4504, Validation Loss=2.1308, Validation acc=0.4457\n",
            "Epoch 8: Train Loss=1.2675, Validation Loss=2.2392, Validation acc=0.4350\n",
            "Epoch 9: Train Loss=1.0857, Validation Loss=2.3459, Validation acc=0.4407\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=256 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=4.0067, Validation Loss=5.7237, Validation acc=0.0207\n",
            "Epoch 1: Train Loss=3.4804, Validation Loss=3.9259, Validation acc=0.1116\n",
            "Epoch 2: Train Loss=3.0972, Validation Loss=4.9839, Validation acc=0.0653\n",
            "Epoch 3: Train Loss=2.7388, Validation Loss=3.8424, Validation acc=0.1462\n",
            "Epoch 4: Train Loss=2.4280, Validation Loss=3.5474, Validation acc=0.1807\n",
            "Epoch 5: Train Loss=2.1700, Validation Loss=3.6816, Validation acc=0.1863\n",
            "Epoch 6: Train Loss=1.9496, Validation Loss=3.3538, Validation acc=0.2501\n",
            "Epoch 7: Train Loss=1.7578, Validation Loss=3.5162, Validation acc=0.1822\n",
            "Epoch 8: Train Loss=1.5859, Validation Loss=3.0997, Validation acc=0.2778\n",
            "Epoch 9: Train Loss=1.4136, Validation Loss=2.8118, Validation acc=0.3341\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=256 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=3.9262, Validation Loss=5.4770, Validation acc=0.0518\n",
            "Epoch 1: Train Loss=3.3171, Validation Loss=3.6543, Validation acc=0.1462\n",
            "Epoch 2: Train Loss=2.8917, Validation Loss=3.3457, Validation acc=0.1925\n",
            "Epoch 3: Train Loss=2.5141, Validation Loss=3.1456, Validation acc=0.2282\n",
            "Epoch 4: Train Loss=2.2107, Validation Loss=3.1899, Validation acc=0.2393\n",
            "Epoch 5: Train Loss=1.9637, Validation Loss=3.0573, Validation acc=0.2640\n",
            "Epoch 6: Train Loss=1.7475, Validation Loss=2.5990, Validation acc=0.3614\n",
            "Epoch 7: Train Loss=1.5637, Validation Loss=2.5496, Validation acc=0.3671\n",
            "Epoch 8: Train Loss=1.3850, Validation Loss=2.5800, Validation acc=0.3587\n",
            "Epoch 9: Train Loss=1.2079, Validation Loss=3.2203, Validation acc=0.2829\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=256 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=3.9003, Validation Loss=3.9479, Validation acc=0.1003\n",
            "Epoch 1: Train Loss=3.3188, Validation Loss=3.8355, Validation acc=0.1338\n",
            "Epoch 2: Train Loss=2.8900, Validation Loss=2.9074, Validation acc=0.2713\n",
            "Epoch 3: Train Loss=2.4951, Validation Loss=2.8202, Validation acc=0.2955\n",
            "Epoch 4: Train Loss=2.1702, Validation Loss=2.5888, Validation acc=0.3440\n",
            "Epoch 5: Train Loss=1.8869, Validation Loss=2.3136, Validation acc=0.3949\n",
            "Epoch 6: Train Loss=1.6732, Validation Loss=2.3487, Validation acc=0.4048\n",
            "Epoch 7: Train Loss=1.4636, Validation Loss=2.1957, Validation acc=0.4319\n",
            "Epoch 8: Train Loss=1.2725, Validation Loss=2.2773, Validation acc=0.4310\n",
            "Epoch 9: Train Loss=1.0866, Validation Loss=2.3157, Validation acc=0.4322\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=512 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=4.1909, Validation Loss=4.5220, Validation acc=0.0538\n",
            "Epoch 1: Train Loss=3.7442, Validation Loss=4.2520, Validation acc=0.0571\n",
            "Epoch 2: Train Loss=3.4737, Validation Loss=5.3878, Validation acc=0.0387\n",
            "Epoch 3: Train Loss=3.2182, Validation Loss=4.2008, Validation acc=0.1052\n",
            "Epoch 4: Train Loss=2.9579, Validation Loss=4.8594, Validation acc=0.0700\n",
            "Epoch 5: Train Loss=2.7382, Validation Loss=3.2607, Validation acc=0.2042\n",
            "Epoch 6: Train Loss=2.4868, Validation Loss=3.9457, Validation acc=0.1453\n",
            "Epoch 7: Train Loss=2.3021, Validation Loss=4.2569, Validation acc=0.1606\n",
            "Epoch 8: Train Loss=2.1169, Validation Loss=3.9902, Validation acc=0.1613\n",
            "Epoch 9: Train Loss=1.9460, Validation Loss=3.7275, Validation acc=0.1835\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=512 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=4.0948, Validation Loss=4.3904, Validation acc=0.0454\n",
            "Epoch 1: Train Loss=3.5748, Validation Loss=4.1540, Validation acc=0.0763\n",
            "Epoch 2: Train Loss=3.2260, Validation Loss=4.5443, Validation acc=0.0611\n",
            "Epoch 3: Train Loss=2.9104, Validation Loss=3.8685, Validation acc=0.1247\n",
            "Epoch 4: Train Loss=2.5884, Validation Loss=3.4911, Validation acc=0.1851\n",
            "Epoch 5: Train Loss=2.3212, Validation Loss=3.9522, Validation acc=0.1425\n",
            "Epoch 6: Train Loss=2.1110, Validation Loss=4.0808, Validation acc=0.1587\n",
            "Epoch 7: Train Loss=1.9183, Validation Loss=3.1498, Validation acc=0.2719\n",
            "Epoch 8: Train Loss=1.7319, Validation Loss=3.1390, Validation acc=0.2616\n",
            "Epoch 9: Train Loss=1.5518, Validation Loss=2.9075, Validation acc=0.2953\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=512 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=4.0258, Validation Loss=4.1654, Validation acc=0.0813\n",
            "Epoch 1: Train Loss=3.4508, Validation Loss=3.5781, Validation acc=0.1481\n",
            "Epoch 2: Train Loss=3.0427, Validation Loss=3.0772, Validation acc=0.2220\n",
            "Epoch 3: Train Loss=2.6690, Validation Loss=3.0785, Validation acc=0.2443\n",
            "Epoch 4: Train Loss=2.3686, Validation Loss=2.6969, Validation acc=0.3059\n",
            "Epoch 5: Train Loss=2.0973, Validation Loss=2.7025, Validation acc=0.3285\n",
            "Epoch 6: Train Loss=1.8779, Validation Loss=2.8207, Validation acc=0.3152\n",
            "Epoch 7: Train Loss=1.6852, Validation Loss=2.6831, Validation acc=0.3446\n",
            "Epoch 8: Train Loss=1.5001, Validation Loss=2.8152, Validation acc=0.3538\n",
            "Epoch 9: Train Loss=1.3393, Validation Loss=3.1178, Validation acc=0.3263\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=1024 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=4.3184, Validation Loss=4.3778, Validation acc=0.0448\n",
            "Epoch 1: Train Loss=3.9507, Validation Loss=6.3444, Validation acc=0.0320\n",
            "Epoch 2: Train Loss=3.7325, Validation Loss=5.1167, Validation acc=0.0406\n",
            "Epoch 3: Train Loss=3.5974, Validation Loss=5.4222, Validation acc=0.0547\n",
            "Epoch 4: Train Loss=3.4260, Validation Loss=6.1227, Validation acc=0.0308\n",
            "Epoch 5: Train Loss=3.2878, Validation Loss=4.8387, Validation acc=0.0518\n",
            "Epoch 6: Train Loss=3.1262, Validation Loss=4.1099, Validation acc=0.1098\n",
            "Epoch 7: Train Loss=2.9841, Validation Loss=4.1206, Validation acc=0.1019\n",
            "Epoch 8: Train Loss=2.8320, Validation Loss=3.7085, Validation acc=0.1408\n",
            "Epoch 9: Train Loss=2.6724, Validation Loss=6.3220, Validation acc=0.0409\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=1024 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=4.2679, Validation Loss=4.2777, Validation acc=0.0571\n",
            "Epoch 1: Train Loss=3.8277, Validation Loss=3.9685, Validation acc=0.0851\n",
            "Epoch 2: Train Loss=3.5828, Validation Loss=4.1162, Validation acc=0.0741\n",
            "Epoch 3: Train Loss=3.3939, Validation Loss=3.9460, Validation acc=0.0959\n",
            "Epoch 4: Train Loss=3.1910, Validation Loss=3.7015, Validation acc=0.1257\n",
            "Epoch 5: Train Loss=2.9982, Validation Loss=4.1884, Validation acc=0.0950\n",
            "Epoch 6: Train Loss=2.8324, Validation Loss=3.7917, Validation acc=0.1383\n",
            "Epoch 7: Train Loss=2.6198, Validation Loss=3.5307, Validation acc=0.1786\n",
            "Epoch 8: Train Loss=2.4433, Validation Loss=5.3994, Validation acc=0.0689\n",
            "Epoch 9: Train Loss=2.3106, Validation Loss=3.6094, Validation acc=0.1672\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.1, batch_size=1024 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=4.2156, Validation Loss=4.1444, Validation acc=0.0694\n",
            "Epoch 1: Train Loss=3.7154, Validation Loss=4.4254, Validation acc=0.0672\n",
            "Epoch 2: Train Loss=3.4514, Validation Loss=3.6750, Validation acc=0.1220\n",
            "Epoch 3: Train Loss=3.1737, Validation Loss=3.8841, Validation acc=0.1337\n",
            "Epoch 4: Train Loss=2.9111, Validation Loss=3.2213, Validation acc=0.2037\n",
            "Epoch 5: Train Loss=2.6705, Validation Loss=3.4877, Validation acc=0.1844\n",
            "Epoch 6: Train Loss=2.4483, Validation Loss=4.0397, Validation acc=0.1492\n",
            "Epoch 7: Train Loss=2.2412, Validation Loss=2.7526, Validation acc=0.3074\n",
            "Epoch 8: Train Loss=2.0568, Validation Loss=2.6189, Validation acc=0.3253\n",
            "Epoch 9: Train Loss=1.8818, Validation Loss=3.0984, Validation acc=0.2661\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=128 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=3.9963, Validation Loss=3.6742, Validation acc=0.1251\n",
            "Epoch 1: Train Loss=3.4084, Validation Loss=3.4857, Validation acc=0.1639\n",
            "Epoch 2: Train Loss=2.9513, Validation Loss=3.2836, Validation acc=0.2063\n",
            "Epoch 3: Train Loss=2.5594, Validation Loss=2.5739, Validation acc=0.3285\n",
            "Epoch 4: Train Loss=2.2509, Validation Loss=2.6367, Validation acc=0.3294\n",
            "Epoch 5: Train Loss=2.0167, Validation Loss=2.3848, Validation acc=0.3809\n",
            "Epoch 6: Train Loss=1.8077, Validation Loss=2.7812, Validation acc=0.3241\n",
            "Epoch 7: Train Loss=1.6255, Validation Loss=2.6789, Validation acc=0.3480\n",
            "Epoch 8: Train Loss=1.4585, Validation Loss=2.4053, Validation acc=0.3854\n",
            "Epoch 9: Train Loss=1.2881, Validation Loss=2.3667, Validation acc=0.3999\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=128 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=3.9085, Validation Loss=3.8698, Validation acc=0.1062\n",
            "Epoch 1: Train Loss=3.2648, Validation Loss=3.4084, Validation acc=0.1802\n",
            "Epoch 2: Train Loss=2.7831, Validation Loss=3.4936, Validation acc=0.1971\n",
            "Epoch 3: Train Loss=2.4218, Validation Loss=2.8287, Validation acc=0.2850\n",
            "Epoch 4: Train Loss=2.1299, Validation Loss=2.6345, Validation acc=0.3346\n",
            "Epoch 5: Train Loss=1.8974, Validation Loss=2.5690, Validation acc=0.3455\n",
            "Epoch 6: Train Loss=1.6897, Validation Loss=2.2883, Validation acc=0.4016\n",
            "Epoch 7: Train Loss=1.4971, Validation Loss=2.2164, Validation acc=0.4207\n",
            "Epoch 8: Train Loss=1.3273, Validation Loss=2.2591, Validation acc=0.4227\n",
            "Epoch 9: Train Loss=1.1540, Validation Loss=2.2655, Validation acc=0.4445\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=128 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=3.9122, Validation Loss=3.6752, Validation acc=0.1195\n",
            "Epoch 1: Train Loss=3.3072, Validation Loss=3.2344, Validation acc=0.2044\n",
            "Epoch 2: Train Loss=2.8385, Validation Loss=3.1798, Validation acc=0.2274\n",
            "Epoch 3: Train Loss=2.4366, Validation Loss=2.5766, Validation acc=0.3331\n",
            "Epoch 4: Train Loss=2.1107, Validation Loss=2.3565, Validation acc=0.3813\n",
            "Epoch 5: Train Loss=1.8551, Validation Loss=2.2186, Validation acc=0.4133\n",
            "Epoch 6: Train Loss=1.6332, Validation Loss=2.2049, Validation acc=0.4321\n",
            "Epoch 7: Train Loss=1.4224, Validation Loss=2.0744, Validation acc=0.4582\n",
            "Epoch 8: Train Loss=1.2240, Validation Loss=2.1868, Validation acc=0.4489\n",
            "Epoch 9: Train Loss=1.0562, Validation Loss=2.2373, Validation acc=0.4499\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=256 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=4.1547, Validation Loss=3.9207, Validation acc=0.1038\n",
            "Epoch 1: Train Loss=3.6670, Validation Loss=4.1473, Validation acc=0.0710\n",
            "Epoch 2: Train Loss=3.3561, Validation Loss=3.4449, Validation acc=0.1675\n",
            "Epoch 3: Train Loss=3.0431, Validation Loss=3.4129, Validation acc=0.1807\n",
            "Epoch 4: Train Loss=2.7615, Validation Loss=3.5598, Validation acc=0.1742\n",
            "Epoch 5: Train Loss=2.5032, Validation Loss=3.1370, Validation acc=0.2186\n",
            "Epoch 6: Train Loss=2.2844, Validation Loss=3.3576, Validation acc=0.2044\n",
            "Epoch 7: Train Loss=2.0830, Validation Loss=2.9919, Validation acc=0.2766\n",
            "Epoch 8: Train Loss=1.9080, Validation Loss=2.7750, Validation acc=0.3078\n",
            "Epoch 9: Train Loss=1.7447, Validation Loss=2.7655, Validation acc=0.3125\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=256 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=4.0362, Validation Loss=3.9851, Validation acc=0.0922\n",
            "Epoch 1: Train Loss=3.4922, Validation Loss=3.6920, Validation acc=0.1324\n",
            "Epoch 2: Train Loss=3.0910, Validation Loss=3.5169, Validation acc=0.1750\n",
            "Epoch 3: Train Loss=2.7219, Validation Loss=3.1439, Validation acc=0.2234\n",
            "Epoch 4: Train Loss=2.4253, Validation Loss=4.0707, Validation acc=0.1305\n",
            "Epoch 5: Train Loss=2.1635, Validation Loss=2.6889, Validation acc=0.3119\n",
            "Epoch 6: Train Loss=1.9580, Validation Loss=2.6612, Validation acc=0.3339\n",
            "Epoch 7: Train Loss=1.7590, Validation Loss=2.5333, Validation acc=0.3571\n",
            "Epoch 8: Train Loss=1.5887, Validation Loss=2.7344, Validation acc=0.3272\n",
            "Epoch 9: Train Loss=1.4222, Validation Loss=3.4182, Validation acc=0.2856\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=256 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=3.9735, Validation Loss=4.1801, Validation acc=0.0788\n",
            "Epoch 1: Train Loss=3.3290, Validation Loss=3.4526, Validation acc=0.1735\n",
            "Epoch 2: Train Loss=2.8476, Validation Loss=3.0543, Validation acc=0.2327\n",
            "Epoch 3: Train Loss=2.4465, Validation Loss=3.3251, Validation acc=0.2211\n",
            "Epoch 4: Train Loss=2.1513, Validation Loss=2.4274, Validation acc=0.3684\n",
            "Epoch 5: Train Loss=1.8987, Validation Loss=2.6015, Validation acc=0.3507\n",
            "Epoch 6: Train Loss=1.6747, Validation Loss=2.2916, Validation acc=0.4068\n",
            "Epoch 7: Train Loss=1.4786, Validation Loss=2.2412, Validation acc=0.4240\n",
            "Epoch 8: Train Loss=1.2955, Validation Loss=2.3520, Validation acc=0.4132\n",
            "Epoch 9: Train Loss=1.1075, Validation Loss=2.2748, Validation acc=0.4327\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=512 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=4.2908, Validation Loss=4.1279, Validation acc=0.0717\n",
            "Epoch 1: Train Loss=3.9111, Validation Loss=4.0678, Validation acc=0.0835\n",
            "Epoch 2: Train Loss=3.6678, Validation Loss=4.5467, Validation acc=0.0721\n",
            "Epoch 3: Train Loss=3.4537, Validation Loss=4.3255, Validation acc=0.0839\n",
            "Epoch 4: Train Loss=3.2516, Validation Loss=4.0590, Validation acc=0.1051\n",
            "Epoch 5: Train Loss=3.0449, Validation Loss=3.3732, Validation acc=0.1931\n",
            "Epoch 6: Train Loss=2.8487, Validation Loss=4.4531, Validation acc=0.1028\n",
            "Epoch 7: Train Loss=2.6740, Validation Loss=4.0449, Validation acc=0.1115\n",
            "Epoch 8: Train Loss=2.5150, Validation Loss=3.7722, Validation acc=0.1736\n",
            "Epoch 9: Train Loss=2.3533, Validation Loss=4.1799, Validation acc=0.1144\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=512 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=4.2168, Validation Loss=4.0585, Validation acc=0.0857\n",
            "Epoch 1: Train Loss=3.7417, Validation Loss=3.9002, Validation acc=0.0997\n",
            "Epoch 2: Train Loss=3.4490, Validation Loss=4.2976, Validation acc=0.0776\n",
            "Epoch 3: Train Loss=3.1839, Validation Loss=3.9213, Validation acc=0.1097\n",
            "Epoch 4: Train Loss=2.9255, Validation Loss=4.0018, Validation acc=0.1274\n",
            "Epoch 5: Train Loss=2.6774, Validation Loss=3.7462, Validation acc=0.1469\n",
            "Epoch 6: Train Loss=2.4652, Validation Loss=3.9777, Validation acc=0.1268\n",
            "Epoch 7: Train Loss=2.2661, Validation Loss=3.7700, Validation acc=0.1624\n",
            "Epoch 8: Train Loss=2.0991, Validation Loss=3.7597, Validation acc=0.1686\n",
            "Epoch 9: Train Loss=1.9265, Validation Loss=3.5374, Validation acc=0.1983\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=512 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=4.1176, Validation Loss=4.4304, Validation acc=0.0673\n",
            "Epoch 1: Train Loss=3.5533, Validation Loss=4.5022, Validation acc=0.0739\n",
            "Epoch 2: Train Loss=3.1984, Validation Loss=3.6642, Validation acc=0.1373\n",
            "Epoch 3: Train Loss=2.8274, Validation Loss=3.5355, Validation acc=0.1835\n",
            "Epoch 4: Train Loss=2.4902, Validation Loss=2.8808, Validation acc=0.2838\n",
            "Epoch 5: Train Loss=2.2276, Validation Loss=2.7014, Validation acc=0.3004\n",
            "Epoch 6: Train Loss=1.9749, Validation Loss=2.6591, Validation acc=0.3274\n",
            "Epoch 7: Train Loss=1.7832, Validation Loss=2.6096, Validation acc=0.3434\n",
            "Epoch 8: Train Loss=1.6020, Validation Loss=2.4104, Validation acc=0.3846\n",
            "Epoch 9: Train Loss=1.4181, Validation Loss=2.9099, Validation acc=0.3420\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=1024 and , momentum = 0.5 : \n",
            "Epoch 0: Train Loss=4.4342, Validation Loss=4.4740, Validation acc=0.0395\n",
            "Epoch 1: Train Loss=4.1352, Validation Loss=4.2905, Validation acc=0.0580\n",
            "Epoch 2: Train Loss=3.9422, Validation Loss=4.0730, Validation acc=0.0846\n",
            "Epoch 3: Train Loss=3.7915, Validation Loss=4.1064, Validation acc=0.0700\n",
            "Epoch 4: Train Loss=3.6650, Validation Loss=4.3153, Validation acc=0.0577\n",
            "Epoch 5: Train Loss=3.5708, Validation Loss=4.8341, Validation acc=0.0570\n",
            "Epoch 6: Train Loss=3.4593, Validation Loss=4.5297, Validation acc=0.0659\n",
            "Epoch 7: Train Loss=3.3357, Validation Loss=4.0444, Validation acc=0.0868\n",
            "Epoch 8: Train Loss=3.2380, Validation Loss=4.1094, Validation acc=0.0864\n",
            "Epoch 9: Train Loss=3.1292, Validation Loss=6.5347, Validation acc=0.0414\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=1024 and , momentum = 0.7 : \n",
            "Epoch 0: Train Loss=4.3752, Validation Loss=4.3361, Validation acc=0.0548\n",
            "Epoch 1: Train Loss=4.0323, Validation Loss=5.0021, Validation acc=0.0407\n",
            "Epoch 2: Train Loss=3.8117, Validation Loss=4.1167, Validation acc=0.0797\n",
            "Epoch 3: Train Loss=3.6239, Validation Loss=6.6189, Validation acc=0.0220\n",
            "Epoch 4: Train Loss=3.4910, Validation Loss=3.9869, Validation acc=0.0844\n",
            "Epoch 5: Train Loss=3.3566, Validation Loss=5.0495, Validation acc=0.0418\n",
            "Epoch 6: Train Loss=3.2079, Validation Loss=3.5585, Validation acc=0.1576\n",
            "Epoch 7: Train Loss=3.0372, Validation Loss=3.9020, Validation acc=0.1438\n",
            "Epoch 8: Train Loss=2.9087, Validation Loss=6.8546, Validation acc=0.0484\n",
            "Epoch 9: Train Loss=2.7725, Validation Loss=3.8398, Validation acc=0.1309\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "Training with lr=0.05, batch_size=1024 and , momentum = 0.9 : \n",
            "Epoch 0: Train Loss=4.3340, Validation Loss=4.4739, Validation acc=0.0298\n",
            "Epoch 1: Train Loss=3.8459, Validation Loss=3.7305, Validation acc=0.1259\n",
            "Epoch 2: Train Loss=3.5341, Validation Loss=5.3827, Validation acc=0.0564\n",
            "Epoch 3: Train Loss=3.2870, Validation Loss=5.4514, Validation acc=0.0439\n",
            "Epoch 4: Train Loss=3.0664, Validation Loss=3.7984, Validation acc=0.1393\n",
            "Epoch 5: Train Loss=2.7997, Validation Loss=3.2306, Validation acc=0.2023\n",
            "Epoch 6: Train Loss=2.5658, Validation Loss=4.3835, Validation acc=0.1410\n",
            "Epoch 7: Train Loss=2.3581, Validation Loss=2.9250, Validation acc=0.2600\n",
            "Epoch 8: Train Loss=2.1794, Validation Loss=3.6383, Validation acc=0.2095\n",
            "Epoch 9: Train Loss=2.0116, Validation Loss=3.2708, Validation acc=0.2464\n",
            "***********************************************************\n",
            "***********************************************************\n",
            "***********************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define for now a batchsize and learning rate to test\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "TEST_BATCH_SIZE = 1024\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "fjYRd0y-YJ4R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2)\n",
        "\n",
        "valid_dataloader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=TEST_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=TEST_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2)"
      ],
      "metadata": {
        "id": "kowXxW0yYFxS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "uMeA9WqlYoHN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1qov44_WcrId"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2CZvQZMR_-i"
      },
      "source": [
        "### Example of a simple CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SpgnMo5nR_-i",
        "outputId": "d808bbce-9d22-46f8-b83d-554b07a6ee8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters:  556708\n"
          ]
        }
      ],
      "source": [
        "class TinyNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TinyNet, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = torch.nn.Linear(8*8*64, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.conv1(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, 2)\n",
        "        x = torch.nn.functional.relu(self.conv2(x))\n",
        "        x = torch.nn.functional.max_pool2d(x, 2)\n",
        "        x = x.view(-1, 8*8*64)\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Model parameters: \", sum(p.numel() for p in TinyNet().parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actual CNN Model**"
      ],
      "metadata": {
        "id": "ARcW9QwoTAfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CorrectBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        identity = x                         # save original input\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        shortcut = self.shortcut(identity)   # apply shortcut to original x\n",
        "        out = out + shortcut                 # residual addition\n",
        "        out = F.relu(out)\n",
        "\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "tEGsz4lES-qC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the model\n",
        "model = ResNet(block=CorrectBlock, num_blocks=[2,2,2,2]).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# train the ResNet\n",
        "\n",
        "train_losses, valid_losses, valid_accs =fit(\n",
        "        model,\n",
        "        train_dataloader = train_dataloader,\n",
        "        optimizer = optimizer,\n",
        "        epochs = 35,\n",
        "        device = device\n",
        "    )\n",
        "\n",
        "plot_losses(train_losses)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B6dLf86LUj4J",
        "outputId": "4311dc14-3853-4450-ff9f-171794353f46"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss=4.6444\n",
            "Epoch 1: Train Loss=4.1242\n",
            "Epoch 2: Train Loss=3.8589\n",
            "Epoch 3: Train Loss=3.6668\n",
            "Epoch 4: Train Loss=3.4654\n",
            "Epoch 5: Train Loss=3.2413\n",
            "Epoch 6: Train Loss=3.0996\n",
            "Epoch 7: Train Loss=2.8849\n",
            "Epoch 8: Train Loss=2.6955\n",
            "Epoch 9: Train Loss=2.4982\n",
            "Epoch 10: Train Loss=2.2634\n",
            "Epoch 11: Train Loss=2.0514\n",
            "Epoch 12: Train Loss=1.8115\n",
            "Epoch 13: Train Loss=1.6342\n",
            "Epoch 14: Train Loss=1.3370\n",
            "Epoch 15: Train Loss=1.1202\n",
            "Epoch 16: Train Loss=0.7756\n",
            "Epoch 17: Train Loss=0.4943\n",
            "Epoch 18: Train Loss=0.3247\n",
            "Epoch 19: Train Loss=0.2017\n",
            "Epoch 20: Train Loss=0.1437\n",
            "Epoch 21: Train Loss=0.1129\n",
            "Epoch 22: Train Loss=0.1041\n",
            "Epoch 23: Train Loss=0.0816\n",
            "Epoch 24: Train Loss=0.0394\n",
            "Epoch 25: Train Loss=0.0448\n",
            "Epoch 26: Train Loss=0.0630\n",
            "Epoch 27: Train Loss=0.0866\n",
            "Epoch 28: Train Loss=0.0632\n",
            "Epoch 29: Train Loss=0.0942\n",
            "Epoch 30: Train Loss=0.0925\n",
            "Epoch 31: Train Loss=0.1078\n",
            "Epoch 32: Train Loss=0.1014\n",
            "Epoch 33: Train Loss=0.0569\n",
            "Epoch 34: Train Loss=0.0753\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM0Lkzp8R_-i"
      },
      "source": [
        "### Example of basic training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSKbYzc3R_-i",
        "outputId": "fa0b6f9d-a28f-4877-c8a3-2eaed2f61bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 4.6038\n",
            "Epoch [2/10], Loss: 4.5989\n",
            "Epoch [3/10], Loss: 4.5938\n",
            "Epoch [4/10], Loss: 4.5835\n",
            "Epoch [5/10], Loss: 4.6095\n",
            "Epoch [6/10], Loss: 4.5991\n",
            "Epoch [7/10], Loss: 4.5833\n",
            "Epoch [8/10], Loss: 4.5818\n",
            "Epoch [9/10], Loss: 4.5808\n",
            "Epoch [10/10], Loss: 4.5845\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = TinyNet()\n",
        "model.to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "for epoch in range(10):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 10, loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8FkIfyYcR_-i",
        "outputId": "36c4e08e-9b92-411e-f7c3-a26220723c72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 556708 parameters\n",
            "\u001b[1m\u001b[91mAccuracy on the test set: 2.6%\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# save the model on a file\n",
        "torch.save(model.state_dict(), 'tiny_net.pt')\n",
        "\n",
        "loaded_model = TinyNet()\n",
        "loaded_model.load_state_dict(torch.load('tiny_net.pt', weights_only=True))\n",
        "evaluate(loaded_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model on a file\n",
        "torch.save(model.state_dict(), 'res_net.pt')\n",
        "\n",
        "loaded_model_1 = ResNet(block=CorrectBlock, num_blocks=[2,2,2,2])\n",
        "loaded_model_1.load_state_dict(torch.load('res_net.pt', weights_only=True))\n",
        "evaluate(loaded_model_1)"
      ],
      "metadata": {
        "id": "qUHVQm0eh4tW",
        "outputId": "503b7eca-8a03-44a5-fcbc-58d7bfa38d0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 11220132 parameters\n",
            "The model has too many parameters! Not allowed to evaluate.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}